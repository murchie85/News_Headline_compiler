{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLINE \n",
    "\n",
    "\n",
    "- Run job three times per day \n",
    "- Gets current date \n",
    "- Checks current date folder \n",
    "- compares news array to written files\n",
    "- If news array has items that are not in written files\n",
    "- Regenerate news array with missing items\n",
    "- call news api, get news for updated news array\n",
    "- Save to json  \n",
    "- Also save all files to csv (can overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2019-12-31\n",
      "Dir Already exists\n",
      "\n",
      "The news sources already obtained for this day are: \n",
      "['cnn', 'the-new-york-times', 'al-jazeera-english', 'bbc-news', 'independent', 'fox-news', 'abc-news']\n",
      "\n",
      "the number of saved sources are: \n",
      "7\n",
      "removing cnn\n",
      "removing the-new-york-times\n",
      "removing al-jazeera-english\n",
      "removing bbc-news\n",
      "removing independent\n",
      "removing fox-news\n",
      "removing abc-news\n",
      "The sources still required are ['mirror', 'metro', 'daily-mail', 'Theguardian.com', 'Sky.com']\n",
      "\n",
      "PULLING NEWS HEADLINES - PLEASE WAIT .... \n",
      "\n",
      "processing mirror headlines\n",
      "Request for the mirror news source is empty, skipping\n",
      "\n",
      "processing metro headlines\n",
      "Request for the metro news source is empty, skipping\n",
      "\n",
      "processing daily-mail headlines\n",
      "Request for the daily-mail news source is empty, skipping\n",
      "\n",
      "processing Theguardian.com headlines\n",
      "Request for the Theguardian.com news source is empty, skipping\n",
      "\n",
      "processing Sky.com headlines\n",
      "Request for the Sky.com news source is empty, skipping\n",
      "\n",
      "PROCESSING COMPLETE\n",
      "number of articles processed are : 60\n"
     ]
    }
   ],
   "source": [
    "# IMPORT STATEMENTS \n",
    "import json\n",
    "import os\n",
    "from datetime import date\n",
    "from newsapi import NewsApiClient\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORT API KEYS\n",
    "f = open(\"../keys/api.txt\", \"r\")\n",
    "keys = f.read()\n",
    "f.close()\n",
    "ACCESS_TOKEN = keys\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "#   PARMS SECTION \n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# READING IN \n",
    "NewsFileNameArray = []                         # A LIST OF FILENAMES \n",
    "data = []                                      # ALL NEWS DATA FROM LOCAL NEWS JSON FILES\n",
    "\n",
    "# REQUESTING NEW DATA\n",
    "newsapi = NewsApiClient(api_key=ACCESS_TOKEN)  # INITIALISING newsapi object\n",
    "news_keyname_array = ['bbc-news', 'abc-news','cnn','fox-news','independent','mirror','metro','daily-mail', 'Theguardian.com' , 'Sky.com', 'the-new-york-times', 'al-jazeera-english']\n",
    "news_array = []\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "#   READING LOCAL FILES \n",
    "#-------------------------------------------------------------------------------------------\n",
    "# GET TODAYS DATE \n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)\n",
    "\n",
    "# CREATE A NEW DIRECTORY FOR TODAY IF NOT EXIST\n",
    "if os.path.isdir(\"data/\" + str(today)):\n",
    "\tprint('Dir Already exists')\n",
    "else:\n",
    "\tos.mkdir(\"data/\" + str(today))\n",
    "\n",
    "print('')\n",
    "\n",
    "# ITERATE THROUGH DATA FOLDER\n",
    "directory = \"data/\" + str(today)\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"): \n",
    "        newsName = filename[:-5]\n",
    "        NewsFileNameArray.append(newsName)\n",
    "        with open(str(directory) + \"/\" + str(filename)) as json_file:\n",
    "            data.append(json.load(json_file))\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "print('The news sources already obtained for this day are: ')\n",
    "print(NewsFileNameArray)\n",
    "print('')\n",
    "print('the number of saved sources are: ')\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "#   UPDATING ARRAY\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#   ONLY MAKE CALLS FOR NEWS SOURCES WE DON'T ALREADY HAVE \n",
    "#   (THIS MEANS UPDATING OUR ARRAY TO REMOVE ALREADY EXISTING SAVED NEWS SOURCES)\n",
    "\n",
    "for item in NewsFileNameArray:\n",
    "    print('removing ' + str(item))\n",
    "    news_keyname_array.remove(str(item))\n",
    "\n",
    "\n",
    "if not news_keyname_array:\n",
    "    print('All data obtained for today, exiting.')\n",
    "    exit()\n",
    "\n",
    "print('The sources still required are ' + str(news_keyname_array))\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "#   REQUESTING MORE NEWS \n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# Init API\n",
    "print('PULLING NEWS HEADLINES - PLEASE WAIT .... ')\n",
    "print('')\n",
    "for item in news_keyname_array:\n",
    "    print('processing ' + str(item + ' headlines'))\n",
    "    news_key = item\n",
    "    json_item = newsapi.get_top_headlines(sources=news_key)\n",
    "    if json_item['totalResults'] == 0:\n",
    "        print(\"Request for the \" + str(item) + \" news source is empty, skipping\")\n",
    "        print('')\n",
    "        continue\n",
    "    news_array.append(json_item)\n",
    "    print('COMPLETE - writing to file .......')\n",
    "    print('')\n",
    "    file_name= str(\"data/\" + str(today) + \"/\" + str(news_key) + \".json\")\n",
    "    with open(file_name, 'w') as fp:\n",
    "        json.dump(json_item, fp)\n",
    "\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------\n",
    "#   BUILDING REPORT \n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# BUILD A PANDAS DATA FRAME \n",
    "df = pd.DataFrame(columns=['source','author','title','description','url', 'requested_date','publishedAt','content'])\n",
    "\n",
    "# CLEAR OUT DATA ARRAY AND REPOPULATE WITH NEW UPDATED JSON FILES (COULD BE MORE EFFICIENT BUT MEH)\n",
    "\n",
    "data = []\n",
    "\n",
    "# ITERATE THROUGH DATA FOLDER\n",
    "directory = \"data/\" + str(today)\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"): \n",
    "        newsName = filename[:-5]\n",
    "        NewsFileNameArray.append(newsName)\n",
    "        with open(str(directory) + \"/\" + str(filename)) as json_file:\n",
    "            data.append(json.load(json_file))\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Iterate through data array and write to csv\n",
    "\n",
    "x = 0 \n",
    "for news_outlet in range (0, len(data)):\n",
    "    for article_number in range (0, len(data[news_outlet]['articles'])):\n",
    "        source         = data[news_outlet]['articles'][article_number]['source']['name']\n",
    "        author         = data[news_outlet]['articles'][article_number]['author']\n",
    "        title          = data[news_outlet]['articles'][article_number]['title']\n",
    "        description    = data[news_outlet]['articles'][article_number]['description']\n",
    "        url            = data[news_outlet]['articles'][article_number]['url']\n",
    "        publishedAt    = data[news_outlet]['articles'][article_number]['publishedAt']\n",
    "        requested_date = today\n",
    "        content        = data[news_outlet]['articles'][article_number]['content']\n",
    "        df = df.append([{ 'source': source, 'author': author, 'title': title, 'description': description, 'url':url, 'publishedAt': publishedAt, 'requested_date': requested_date, 'content': content    }])\n",
    "        x = x + 1 \n",
    "\n",
    "print('PROCESSING COMPLETE')\n",
    "print('number of articles processed are : ' + str(x))\n",
    "\n",
    "        \n",
    "df.to_csv(\"data/\" + str(today) + \"/\" + 'output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT STATEMENTS \n",
    "import json\n",
    "import os\n",
    "from datetime import date\n",
    "from newsapi import NewsApiClient\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORT API KEYS\n",
    "f = open(\"../keys/api.txt\", \"r\")\n",
    "keys = f.read()\n",
    "f.close()\n",
    "ACCESS_TOKEN = keys\n",
    "newsapi = NewsApiClient(api_key=ACCESS_TOKEN)\n",
    "data = newsapi.get_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc-news\n",
      "abc-news-au\n",
      "aftenposten\n",
      "al-jazeera-english\n",
      "ansa\n",
      "argaam\n",
      "ars-technica\n",
      "ary-news\n",
      "associated-press\n",
      "australian-financial-review\n",
      "axios\n",
      "bbc-news\n",
      "bbc-sport\n",
      "bild\n",
      "blasting-news-br\n",
      "bleacher-report\n",
      "bloomberg\n",
      "breitbart-news\n",
      "business-insider\n",
      "business-insider-uk\n",
      "buzzfeed\n",
      "cbc-news\n",
      "cbs-news\n",
      "cnbc\n",
      "cnn\n",
      "cnn-es\n",
      "crypto-coins-news\n",
      "der-tagesspiegel\n",
      "die-zeit\n",
      "el-mundo\n",
      "engadget\n",
      "entertainment-weekly\n",
      "espn\n",
      "espn-cric-info\n",
      "financial-post\n",
      "focus\n",
      "football-italia\n",
      "fortune\n",
      "four-four-two\n",
      "fox-news\n",
      "fox-sports\n",
      "globo\n",
      "google-news\n",
      "google-news-ar\n",
      "google-news-au\n",
      "google-news-br\n",
      "google-news-ca\n",
      "google-news-fr\n",
      "google-news-in\n",
      "google-news-is\n",
      "google-news-it\n",
      "google-news-ru\n",
      "google-news-sa\n",
      "google-news-uk\n",
      "goteborgs-posten\n",
      "gruenderszene\n",
      "hacker-news\n",
      "handelsblatt\n",
      "ign\n",
      "il-sole-24-ore\n",
      "independent\n",
      "infobae\n",
      "info-money\n",
      "la-gaceta\n",
      "la-nacion\n",
      "la-repubblica\n",
      "le-monde\n",
      "lenta\n",
      "lequipe\n",
      "les-echos\n",
      "liberation\n",
      "marca\n",
      "mashable\n",
      "medical-news-today\n",
      "msnbc\n",
      "mtv-news\n",
      "mtv-news-uk\n",
      "national-geographic\n",
      "national-review\n",
      "nbc-news\n",
      "news24\n",
      "new-scientist\n",
      "news-com-au\n",
      "newsweek\n",
      "new-york-magazine\n",
      "next-big-future\n",
      "nfl-news\n",
      "nhl-news\n",
      "nrk\n",
      "politico\n",
      "polygon\n",
      "rbc\n",
      "recode\n",
      "reddit-r-all\n",
      "reuters\n",
      "rt\n",
      "rte\n",
      "rtl-nieuws\n",
      "sabq\n",
      "spiegel-online\n",
      "svenska-dagbladet\n",
      "t3n\n",
      "talksport\n",
      "techcrunch\n",
      "techcrunch-cn\n",
      "techradar\n",
      "the-american-conservative\n",
      "the-globe-and-mail\n",
      "the-hill\n",
      "the-hindu\n",
      "the-huffington-post\n",
      "the-irish-times\n",
      "the-jerusalem-post\n",
      "the-lad-bible\n",
      "the-new-york-times\n",
      "the-next-web\n",
      "the-sport-bible\n",
      "the-times-of-india\n",
      "the-verge\n",
      "the-wall-street-journal\n",
      "the-washington-post\n",
      "the-washington-times\n",
      "time\n",
      "usa-today\n",
      "vice-news\n",
      "wired\n",
      "wired-de\n",
      "wirtschafts-woche\n",
      "xinhua-net\n",
      "ynet\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, len(data['sources'])):\n",
    "    print(data['sources'][x]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc-news'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sources'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
